{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66cb3480-2824-4020-bbe1-417df846df28",
   "metadata": {},
   "source": [
    "Cora and Facebook Page-Page Dataset\n",
    "\n",
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes and this network contains around 5429 links. Each node or a a publication is represented by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary\n",
    "\n",
    "Facebook Page-Page is a page-page graph of verified Facebook sites. Nodes represent official Facebook pages while the links are mutual likes between sites. Node features are extracted from the site descriptions that the page owners created to summarize the purpose of the site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104eb04f-7874-4d8c-8f84-8ef103a75b2d",
   "metadata": {},
   "source": [
    "Inorder to visualize the network we use the torch geometric library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a25fdb-3965-471a-99a0-dfb3a940b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f380dd0-8f82-4680-9db6-f9c4e94220f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Planetoid(root=\".\", name=\"Cora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a036e9-c486-41d5-a4f5-d790177328e9",
   "metadata": {},
   "source": [
    "Let us explore the dataset !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d163ecba-3b33-4e13-83bf-00652b0aeae8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b42de39-08d5-426b-8303-c2c58f0f5e5b",
   "metadata": {},
   "source": [
    "We can see that there 1433 features, 2708 graphs in nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6733bb4-486f-4cc9-bed6-68dac58390f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879cbfd9-0250-4b59-a37b-6fe1bc3451ef",
   "metadata": {},
   "source": [
    "There are seven classes for the nodes. Now let us look into Facebook page-page dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aa8fe0-6bae-49bf-ade6-7f18795a9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import FacebookPagePage\n",
    "facebook_dataset = FacebookPagePage('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbbc99c7-963a-4126-bf9e-4b4e35a0c7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[22470, 128], edge_index=[2, 342004], y=[22470])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "facebook_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69af8d1-6330-4a35-aa0d-51eb5b2e4f3e",
   "metadata": {},
   "source": [
    "One thing to note is that facebook page page dataset does not have train_mask when compared to the Cora dataset, so we have to create the mask randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b163a331-5b38-49d2-af39-86dda16ddbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "facebook_dataset[0].train_mask = range(18000)\n",
    "facebook_dataset[0].val_mask = range(18001, 20000)\n",
    "facebook_dataset[0].test_mask = range(20001, 22470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4d5f164-86f1-4682-9bdc-ee5126d346b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(facebook_dataset[0].is_directed())\n",
    "print(facebook_dataset[0].has_self_loops())\n",
    "print(facebook_dataset[0].has_isolated_nodes())\n",
    "print(facebook_dataset[0].is_coalesced())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3479aae8-24c9-48be-8328-934b512013dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0].is_directed())\n",
    "print(dataset[0].has_self_loops())\n",
    "print(dataset[0].has_isolated_nodes())\n",
    "print(dataset[0].is_coalesced())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946909a-706e-48c6-90b1-e549a99cbca6",
   "metadata": {},
   "source": [
    "So we can see the Facebook page-page graph contains self-loop while the Cora dataset edge indices are sorted and does not have duplicate entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecbc8a4-96bb-4f94-a135-5a33f453f1cf",
   "metadata": {},
   "source": [
    "One way to do node classification will be to use the node features directly for classification without taking into consideration the topology of the network. Other to use Graph Neural netowrk for node classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0841b762-8150-4cad-b5e7-5e2629deddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[0].x.numpy()\n",
    "y=  dataset[0].y.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184613ac-c0d3-4e8b-bfc8-10f75dc9c187",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "277c4cae-9632-4b15-9cd8-37aaf941c0af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 4, ..., 3, 3, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2473203-2539-4a52-b6db-5980ced1eab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = X[dataset[0].train_mask]\n",
    "y_train = y[dataset[0].train_mask]\n",
    "x_val = X[dataset[0].val_mask]\n",
    "y_val = y[dataset[0].val_mask]\n",
    "x_test = X[dataset[0].test_mask]\n",
    "y_test = y[dataset[0].test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8348f346-9ba6-4546-a44f-fa160c4a67f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83939e79-911c-49a6-8fc9-83e51240a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self,inp,hidden,out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp,hidden)\n",
    "        self.linear2 = nn.Linear(hidden,out)\n",
    "    def forward(self,inp):\n",
    "        x = self.linear1(inp)\n",
    "        x = self.linear2(x)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01a10267-a5e2-432e-9354-8e247852dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y_true):\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af4415a1-1676-4154-a850-c9df2b848cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP(dataset.num_features, 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "237c2616-1555-47e2-9d31-ac851413517a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 0.216 | Train Acc: 100.00% | Val Loss: 1.47 | Val Acc: 52.20%\n",
      "Epoch  10 | Train Loss: 0.025 | Train Acc: 100.00% | Val Loss: 1.47 | Val Acc: 50.80%\n",
      "Epoch  20 | Train Loss: 0.009 | Train Acc: 100.00% | Val Loss: 1.50 | Val Acc: 50.60%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset[0].x)\n",
    "    loss = criterion(out[dataset[0].train_mask],dataset[0].y[dataset[0].train_mask])\n",
    "    acc = accuracy(out[dataset[0].train_mask].argmax(dim=1),dataset[0].y[dataset[0].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "            val_loss = criterion(out[dataset[0].val_mask], dataset[0].y[dataset[0].val_mask])\n",
    "            val_acc = accuracy(out[dataset[0].val_mask].argmax(dim=1), dataset[0].y[dataset[0].val_mask])\n",
    "            print(f'Epoch {i:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccd886b2-9830-4f33-a9e2-a032e28489c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(dataset[0].x)\n",
    "test_acc = accuracy(out[dataset[0].test_mask].argmax(dim=1), dataset[0].y[dataset[0].test_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "afdd121a-d515-43ff-9637-3cb3fca73b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  tensor(0.5080)\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1909f2-35ed-48d1-8a2f-39fc0309328e",
   "metadata": {},
   "source": [
    "We are getting around 50% accuracy. Which does not look that good. Now let us try the same in Facebook page page dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff690096-dd51-4d29-a527-8bb8419ae443",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data= facebook_dataset\n",
    "# model = MLP(new_data.num_features, 16, new_data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1337b44b-0f27-482f-ad09-a90c47d23ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = range(18000)\n",
    "val_mask = range(18001, 20000)\n",
    "test_mask = range(20001, 22470)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0386a3f5-270d-42d1-aca3-6d71509da87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 1.486 | Train Acc: 25.49% | Val Loss: 1.50 | Val Acc: 24.71%\n",
      "Epoch  10 | Train Loss: 0.733 | Train Acc: 71.53% | Val Loss: 0.74 | Val Acc: 70.44%\n",
      "Epoch  20 | Train Loss: 0.639 | Train Acc: 74.89% | Val Loss: 0.66 | Val Acc: 73.69%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(new_data[0].x)\n",
    "    loss = criterion(out[train_mask],new_data[0].y[train_mask])\n",
    "    acc = accuracy(out[train_mask].argmax(dim=1),new_data[0].y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "            val_loss = criterion(out[val_mask], new_data[0].y[val_mask])\n",
    "            val_acc = accuracy(out[val_mask].argmax(dim=1), new_data[0].y[val_mask])\n",
    "            print(f'Epoch {i:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66649bfa-afab-4d90-9fcc-7d826c0b66b2",
   "metadata": {},
   "source": [
    "This looks much better. Now let us see how including the topology of the graph improves the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed66aba-6f0c-4d9b-90b7-d0f4dedd3e2e",
   "metadata": {},
   "source": [
    "Now let us build the custom GNN layer. $H=\\tilde{A}^T X W^T$, here $\\tilde{A} = A + I$, ensures that the information from the neigbourhood is aggregated while finding the representation. The I is added to ensure that the central node is also included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61f72e0d-d2b1-48a3-9fdb-12fa55875599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNlayer(nn.Module):\n",
    "    def __init__(self,inp,out):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(inp,out)\n",
    "    def forward(self,x,adj):\n",
    "        x = self.linear1(x)\n",
    "        return torch.sparse.mm(adj,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a56e41fb-c589-4ed7-a769-629a43905f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import to_dense_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e9e0021-7943-464f-9042-9b0a4bed22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_dense_adj(dataset[0].edge_index)[0]\n",
    "adj += torch.eye(len(adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d0d9eb7-b187-48c2-978d-20867903947d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 1., 1.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.],\n",
       "        [0., 0., 0.,  ..., 0., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b343474f-bf43-48be-9ce6-eefa79015d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mymodel(nn.Module):\n",
    "    def __init__(self,inp,hidden,out):\n",
    "        super().__init__()\n",
    "        self.layer1 = GNNlayer(inp,hidden)\n",
    "        self.layer2 = GNNlayer(hidden,out)\n",
    "    def forward(self,x,adj):\n",
    "        x = self.layer1(x,adj)\n",
    "        x = self.layer2(x,adj)\n",
    "        return F.log_softmax(x,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "13839f7b-316d-40c2-a3b7-dd0526458649",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodel(new_data.num_features, 16, new_data.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ae8eef1-f474-4b5d-b5d9-2387e46b3ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_dense_adj(new_data[0].edge_index)[0]\n",
    "adj += torch.eye(len(adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "897e27b9-0cfa-426a-93e2-2f990048f65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 45.475 | Train Acc: 40.23% | Val Loss: 41.26 | Val Acc: 40.27%\n",
      "Epoch  10 | Train Loss: 16.360 | Train Acc: 74.03% | Val Loss: 11.23 | Val Acc: 74.84%\n",
      "Epoch  20 | Train Loss: 5.211 | Train Acc: 81.28% | Val Loss: 3.66 | Val Acc: 82.94%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(new_data[0].x,adj)\n",
    "    loss = criterion(out[train_mask],new_data[0].y[train_mask])\n",
    "    acc = accuracy(out[train_mask].argmax(dim=1),new_data[0].y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "            val_loss = criterion(out[val_mask], new_data[0].y[val_mask])\n",
    "            val_acc = accuracy(out[val_mask].argmax(dim=1), new_data[0].y[val_mask])\n",
    "            print(f'Epoch {i:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5dbb3126-eb45-422b-85ca-461559838c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  tensor(0.8360)\n"
     ]
    }
   ],
   "source": [
    "out = model(new_data[0].x,adj)\n",
    "test_acc = accuracy(out[test_mask].argmax(dim=1), new_data[0].y[test_mask])\n",
    "print(\"Test accuracy: \",test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b09176-556c-4704-9542-4c07e41b710a",
   "metadata": {},
   "source": [
    "😮. The accuracy has increased from 73% to 83% with neighbourhood aggregation using Adjacency matrix. This is very interesting. Now let us try it with cora dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bdd63ca3-cd75-408f-930c-f581ad18e618",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mymodel(dataset.num_features, 16, dataset.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f390d7b3-881e-407c-a6eb-3c723a63b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = to_dense_adj(dataset[0].edge_index)[0]\n",
    "adj += torch.eye(len(adj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "879411d6-d750-473d-8c4c-cb0dc9388478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 2.874 | Train Acc: 47.86% | Val Loss: 2.59 | Val Acc: 41.20%\n",
      "Epoch  10 | Train Loss: 0.892 | Train Acc: 87.14% | Val Loss: 2.62 | Val Acc: 65.60%\n",
      "Epoch  20 | Train Loss: 0.092 | Train Acc: 97.86% | Val Loss: 1.91 | Val Acc: 74.00%\n"
     ]
    }
   ],
   "source": [
    "n_epochs=30\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "for i in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(dataset[0].x,adj)\n",
    "    loss = criterion(out[dataset[0].train_mask],dataset[0].y[dataset[0].train_mask])\n",
    "    acc = accuracy(out[dataset[0].train_mask].argmax(dim=1),dataset[0].y[dataset[0].train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "            val_loss = criterion(out[dataset[0].val_mask], dataset[0].y[dataset[0].val_mask])\n",
    "            val_acc = accuracy(out[dataset[0].val_mask].argmax(dim=1), dataset[0].y[dataset[0].val_mask])\n",
    "            print(f'Epoch {i:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30696bfc-b1dd-4b60-b034-652037505d3a",
   "metadata": {},
   "source": [
    "There is an improvement in the accuracy of the CORA dataset too. This is very interesting. Thus including the node features and graph topology information together will help in improving the accuracy of the model. Thats all for today. Bye!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
